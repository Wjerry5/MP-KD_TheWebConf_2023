{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Load temporal triplet\n",
    "### Step 2. Load multilingual entity alignments\n",
    "### Step 3. Output raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_data = []\n",
    "with open('EventKG/relations_entities_temporal.nq') as f:\n",
    "    lines = f.readlines()\n",
    "    temporal_data = lines\n",
    "\n",
    "entity_file = []\n",
    "with open('EventKG/entities.nq') as f:\n",
    "    lines = f.readlines()\n",
    "    entity_file = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw temporal triplet: (3592450, 6)\n"
     ]
    }
   ],
   "source": [
    "def extract(string):\n",
    "    pattern = re.compile(r'\"(.*)\"@(.*)<')\n",
    "    match = re.findall(pattern, string)\n",
    "    return match\n",
    "\n",
    "temporal_event = dict()\n",
    "for line in temporal_data:\n",
    "    line = line.strip()\n",
    "    line = line.split(' ')\n",
    "    relation = line[0]\n",
    "    # print(line)\n",
    "    # break\n",
    "    if relation not in temporal_event:\n",
    "        # [subject, object, begin time, end time, relation, data source]\n",
    "        temporal_event[relation] = [None, None, None, None, None, None]\n",
    "    if line[1] == '<http://www.w3.org/1999/02/22-rdf-syntax-ns#subject>':\n",
    "        temporal_event[relation][0] = line[2]\n",
    "    elif line[1] == '<http://www.w3.org/1999/02/22-rdf-syntax-ns#object>':\n",
    "        temporal_event[relation][1] = line[2]\n",
    "    elif line[1] == '<http://semanticweb.cs.vu.nl/2009/11/sem/hasBeginTimeStamp>':\n",
    "        temporal_event[relation][2] = line[2].split(\"^^\")[0][1:-1]\n",
    "    elif line[1] == '<http://semanticweb.cs.vu.nl/2009/11/sem/hasEndTimeStamp>':\n",
    "        temporal_event[relation][3] = line[2].split(\"^^\")[0][1:-1]\n",
    "    elif line[1] == '<http://semanticweb.cs.vu.nl/2009/11/sem/roleType>':\n",
    "        temporal_event[relation][4] = line[2]\n",
    "        temporal_event[relation][5] = line[3]\n",
    "\n",
    "temporal_triplet = pd.DataFrame.from_dict(temporal_event, orient='index',\n",
    "                columns=['subject', 'object', 'beginTime', 'endTime', 'relation', 'dataSource'])\n",
    "temporal_triplet = temporal_triplet.dropna()\n",
    "print(\"raw temporal triplet:\", temporal_triplet.shape)\n",
    "\n",
    "\n",
    "languages = set()\n",
    "entity_data = dict()\n",
    "for line in entity_file:\n",
    "    entity_ = line.strip().split(' ')\n",
    "    entity_id = entity_[0]\n",
    "    if entity_id not in entity_data:\n",
    "        # entity_id -> [ID, description, language]\n",
    "        entity_data[entity_id] = [entity_id, None, []]\n",
    "\n",
    "    match = extract(line)\n",
    "    if match:\n",
    "        entity_dscp, entity_lang = match[0][0], match[0][1][:-1]\n",
    "        entity_data[entity_id][1] = entity_dscp\n",
    "        entity_data[entity_id][2].append(entity_lang)\n",
    "        languages.add(entity_lang)\n",
    "for entity_ in entity_data:\n",
    "    entity_data[entity_][2] = ' '.join(entity_data[entity_][2])\n",
    "entity_data = pd.DataFrame.from_dict(entity_data, orient='index', columns=['entityID', 'description', 'language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data:\n",
    "\n",
    "def dump_raw_data(data, data_name):\n",
    "    if not os.path.exists(data_name):\n",
    "        os.mkdir(data_name)\n",
    "    with open(os.path.join(data_name, 'raw_data.pickle'), 'wb') as handle:\n",
    "        pickle.dump(data, handle)\n",
    "    \n",
    "def load_raw_data(data_name):\n",
    "    with open(os.path.join(data_name, 'raw_data.pickle'), 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "    return data\n",
    "\n",
    "def dump_split_data(data, data_name):\n",
    "    if not os.path.exists(data_name):\n",
    "        os.mkdir(data_name)\n",
    "    with open(os.path.join(data_name, 'split_data.pickle'), 'wb') as handle:\n",
    "        pickle.dump(data, handle)\n",
    "    \n",
    "def load_split_data(data_name):\n",
    "    with open(os.path.join(data_name, 'split_data.pickle'), 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "    return data\n",
    "\n",
    "def dump_task_data(data, data_name):\n",
    "    if not os.path.exists(data_name):\n",
    "        os.mkdir(data_name)\n",
    "    with open(os.path.join(data_name, 'task_data.pickle'), 'wb') as handle:\n",
    "        pickle.dump(data, handle)\n",
    "    \n",
    "def load_task_data(data_name):\n",
    "    with open(os.path.join(data_name, 'task_data.pickle'), 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "    return data\n",
    "\n",
    "def filter_triplet(data_name, start_time, end_time, frequency = 20):\n",
    "    # filter the triplet based on the time interval and frequency\n",
    "    # data_name: the name of the data source\n",
    "    # start_time: the start time of the time interval\n",
    "    # end_time: the end time of the time interval\n",
    "    # frequency: the frequency of triplet\n",
    "    # return: the filtered triplet\n",
    "    print(\"start preparing %s data\" % data_name)\n",
    "    filtered_triplet = temporal_triplet[(temporal_triplet['dataSource'].str.contains(data_name)) & \n",
    "                temporal_triplet['subject'].str.contains(\"entity\") & temporal_triplet['object'].str.contains(\"entity\")]\n",
    "    filtered_triplet = filtered_triplet[(filtered_triplet['beginTime'] > start_time) & (filtered_triplet['endTime'] < end_time)]\n",
    "    print(\"raw temporal triplet:\", len(filtered_triplet))\n",
    "    filtered_triplet = filtered_triplet.drop_duplicates()\n",
    "    entity_frequency = dict()\n",
    "    for entity_ in ['subject', 'object']:\n",
    "        for index, row in filtered_triplet.groupby(entity_).size().items():\n",
    "            if index not in entity_frequency:\n",
    "                entity_frequency[index] = 0\n",
    "            entity_frequency[index] += row\n",
    "    frequent_entity = [key for key, value in entity_frequency.items() if value >= frequency]\n",
    "    filtered_triplet = filtered_triplet[filtered_triplet['subject'].isin(frequent_entity) \n",
    "                    & filtered_triplet['object'].isin(frequent_entity)]\n",
    "    print(\"filtered temporal triplet:\", filtered_triplet.shape)\n",
    "    print(\"frequent entity:\", len(frequent_entity))\n",
    "    return filtered_triplet, frequent_entity\n",
    "\n",
    "def language_partition(data):\n",
    "    # partition the data based on the languages\n",
    "    # data: the data to be partitioned\n",
    "    # entity_data: the entity info\n",
    "    # languages: the languages to be partitioned\n",
    "    # return: the partitioned data\n",
    "    data_partition = dict()\n",
    "    for lang_ in languages:\n",
    "        entity_lang_set = set(entity_data[entity_data['language'].str.contains(lang_)].entityID)\n",
    "        temporal_triplet_lang = data[data['subject'].isin(entity_lang_set) & data['object'].isin(entity_lang_set)]\n",
    "        relations_lang = set(temporal_triplet_lang.relation.unique())\n",
    "        entities_lang = set(temporal_triplet_lang.subject.unique()) | set(temporal_triplet_lang.object.unique())\n",
    "        print(\"languages: \", lang_) \n",
    "        print(\"number of triplets: \", len(temporal_triplet_lang))\n",
    "        print(\"number of relations: \", len(relations_lang))\n",
    "        print(\"number of entities: \", len(entities_lang))\n",
    "        data_partition[lang_] = temporal_triplet_lang\n",
    "    return data_partition\n",
    "\n",
    "def prepare_data(data_name, start_time = \"1980\", end_time = \"2022\", frequency = 10):\n",
    "    data, _ = filter_triplet(data_name, start_time, end_time, frequency)\n",
    "    data_partition = language_partition(data)\n",
    "    return data_partition\n",
    "\n",
    "val_time = \"2005-01-01\"\n",
    "test_time = \"2010-01-01\"\n",
    "\n",
    "def string_to_datetime(string):\n",
    "    try:\n",
    "        element = datetime.datetime.strptime(string, \"%Y-%m-%d\")\n",
    "        return datetime.datetime.timestamp(element)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def datetime_to_index(timestamp, min_time, max_time, num_time_interval):\n",
    "    time_interval = (max_time - min_time) / (num_time_interval)\n",
    "    return int((timestamp - min_time) / (time_interval + 1e-6))\n",
    "\n",
    "def convert_time(data, min_time, max_time, time_steps, val_split = 28, test_split = 32, frac = 0.5):\n",
    "    # convert the time to the time steps\n",
    "    # data: the data to be converted\n",
    "    # min_time: the minimum time of the data\n",
    "    # max_time: the maximum time of the data\n",
    "    # val_split: the time to split the data into validation set\n",
    "    # test_split: the time to split the data into test set\n",
    "    # time_steps: the time steps of the data\n",
    "    # frac: the fraction of the data to be used for validation set\n",
    "    # return: the converted data\n",
    "    data = data.dropna()\n",
    "    data['beginTime'] = data['beginTime'].apply(datetime_to_index, args = (min_time, max_time, time_steps))\n",
    "    data['endTime'] = data['endTime'].apply(datetime_to_index, args = (min_time, max_time, time_steps))\n",
    "    convert_data = {\"raw\": [], \"train\": [], \"val\": [], \"int\": [], \"ext\": []}\n",
    "    for index, row in data.iterrows():\n",
    "        for time_ in range(row['beginTime'], row['endTime'] + 1):\n",
    "            convert_data['raw'].append((row['subject'], row['relation'], row['object'], time_))\n",
    "            if time_ < val_split:\n",
    "                if random.random() < frac:\n",
    "                    convert_data['train'].append((row['subject'], row['relation'], row['object'], time_))\n",
    "                else:\n",
    "                    convert_data['int'].append((row['subject'], row['relation'], row['object'], time_))\n",
    "            elif time_ < test_split:\n",
    "                convert_data['val'].append((row['subject'], row['relation'], row['object'], time_))\n",
    "            else:\n",
    "                convert_data['ext'].append((row['subject'], row['relation'], row['object'], time_))\n",
    "    relation_set = set([item[1] for item in convert_data['train']]) | set([item[1] for item in convert_data['int']])\n",
    "    entity_set = set([item[0] for item in convert_data['train']]) | set([item[0] for item in convert_data['int']]) | set([item[2] for item in convert_data['train']]) | set([item[2] for item in convert_data['int']])\n",
    "\n",
    "    for key in convert_data:\n",
    "        convert_data[key] = pd.DataFrame(convert_data[key], columns = ['subject', 'relation', 'object', 'time']).drop_duplicates()\n",
    "        convert_data[key] = convert_data[key][convert_data[key].relation.isin(relation_set)]\n",
    "        convert_data[key] = convert_data[key][convert_data[key].subject.isin(entity_set) & convert_data[key].object.isin(entity_set)]\n",
    "    \n",
    "    print(\"number of entities: \", len(entity_set))\n",
    "    print(\"number of relations: \", len(relation_set))\n",
    "    print(\"number of train triplets: \", len(convert_data['train']))\n",
    "    print(\"number of int triplets: \", len(convert_data['int']))\n",
    "    print(\"number of val triplets: \", len(convert_data['val']))\n",
    "    print(\"number of ext triplets: \", len(convert_data['ext']))\n",
    "    print(\"total time step: \", time_steps)\n",
    "    print(\"number of train time step: \", len(convert_data['train'].time.unique()))\n",
    "    print(\"number of ext time step: \", len(convert_data['ext'].time.unique()))\n",
    "    print(\"number of val time step: \", len(convert_data['val'].time.unique()))\n",
    "    return convert_data\n",
    "    \n",
    "def split_data(input_data, time_steps):\n",
    "    data = input_data.copy()\n",
    "    split_data = dict()\n",
    "    val_split = string_to_datetime(val_time)\n",
    "    test_split = string_to_datetime(test_time)\n",
    "    for lang_ in data:\n",
    "        data[lang_]['beginTime'] = data[lang_]['beginTime'].apply(string_to_datetime)\n",
    "        data[lang_]['endTime'] = data[lang_]['endTime'].apply(string_to_datetime)\n",
    "    min_time, max_time = data['en']['beginTime'].min(), data['en']['endTime'].max()\n",
    "    print(min_time, max_time)\n",
    "    for lang_ in data:\n",
    "        print(\"=========preparing {} data============\".format(lang_))\n",
    "        split_data[lang_] = convert_time(data[lang_], min_time, max_time, time_steps)\n",
    "    return split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preparing wiki data\n",
      "raw temporal triplet: 2185579\n",
      "filtered temporal triplet: (288133, 6)\n",
      "frequent entity: 48418\n",
      "languages:  pl\n",
      "number of triplets:  121671\n",
      "number of relations:  91\n",
      "number of entities:  17794\n",
      "languages:  bg\n",
      "number of triplets:  15934\n",
      "number of relations:  68\n",
      "number of entities:  3887\n",
      "languages:  da\n",
      "number of triplets:  101741\n",
      "number of relations:  86\n",
      "number of entities:  16692\n",
      "languages:  hr\n",
      "number of triplets:  12880\n",
      "number of relations:  70\n",
      "number of entities:  3496\n",
      "languages:  nl\n",
      "number of triplets:  255931\n",
      "number of relations:  106\n",
      "number of entities:  32776\n",
      "languages:  de\n",
      "number of triplets:  221043\n",
      "number of relations:  104\n",
      "number of entities:  29093\n",
      "languages:  ro\n",
      "number of triplets:  38241\n",
      "number of relations:  77\n",
      "number of entities:  7587\n",
      "languages:  it\n",
      "number of triplets:  192246\n",
      "number of relations:  94\n",
      "number of entities:  25124\n",
      "languages:  no\n",
      "number of triplets:  27229\n",
      "number of relations:  72\n",
      "number of entities:  6196\n",
      "languages:  ru\n",
      "number of triplets:  121016\n",
      "number of relations:  99\n",
      "number of entities:  18132\n",
      "languages:  pt\n",
      "number of triplets:  109834\n",
      "number of relations:  91\n",
      "number of entities:  16727\n",
      "languages:  en\n",
      "number of triplets:  287630\n",
      "number of relations:  120\n",
      "number of entities:  37517\n",
      "languages:  es\n",
      "number of triplets:  264493\n",
      "number of relations:  104\n",
      "number of entities:  33402\n",
      "languages:  sl\n",
      "number of triplets:  44196\n",
      "number of relations:  68\n",
      "number of entities:  14732\n",
      "languages:  fr\n",
      "number of triplets:  269808\n",
      "number of relations:  105\n",
      "number of entities:  34260\n"
     ]
    }
   ],
   "source": [
    "wiki_data = prepare_data('wiki', \"1980\", \"2022\", frequency = 10)\n",
    "# yago_data = prepare_data('yago', \"1980\", \"2022\", frequency = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_raw_data(wiki_data, 'wiki')\n",
    "#dump_raw_data(yago_data, 'yago')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = load_raw_data('wiki')\n",
    "#yago_data = load_raw_data('yago')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315532800.0 1640908800.0\n",
      "=========preparing pl data============\n",
      "number of entities:  16706\n",
      "number of relations:  83\n",
      "number of train triplets:  139685\n",
      "number of int triplets:  139490\n",
      "number of val triplets:  63521\n",
      "number of ext triplets:  35345\n",
      "total time step:  40\n",
      "number of train time step:  28\n",
      "number of ext time step:  8\n",
      "number of val time step:  4\n",
      "=========preparing bg data============\n",
      "number of entities:  3508\n",
      "number of relations:  61\n",
      "number of train triplets:  19740\n",
      "number of int triplets:  19754\n",
      "number of val triplets:  9556\n",
      "number of ext triplets:  6358\n",
      "total time step:  40\n",
      "number of train time step:  28\n",
      "number of ext time step:  8\n",
      "number of val time step:  4\n",
      "=========preparing da data============\n",
      "number of entities:  15710\n",
      "number of relations:  78\n",
      "number of train triplets:  120913\n",
      "number of int triplets:  120683\n",
      "number of val triplets:  50725\n",
      "number of ext triplets:  26010\n",
      "total time step:  40\n",
      "number of train time step:  28\n",
      "number of ext time step:  8\n",
      "number of val time step:  4\n",
      "=========preparing hr data============\n",
      "number of entities:  3161\n",
      "number of relations:  63\n",
      "number of train triplets:  16873\n",
      "number of int triplets:  16838\n",
      "number of val triplets:  7743\n",
      "number of ext triplets:  5186\n",
      "total time step:  40\n",
      "number of train time step:  28\n",
      "number of ext time step:  8\n",
      "number of val time step:  4\n",
      "=========preparing nl data============\n",
      "number of entities:  30872\n",
      "number of relations:  93\n",
      "number of train triplets:  277180\n",
      "number of int triplets:  275818\n",
      "number of val triplets:  131327\n",
      "number of ext triplets:  63535\n",
      "total time step:  40\n",
      "number of train time step:  28\n",
      "number of ext time step:  8\n",
      "number of val time step:  4\n",
      "=========preparing de data============\n",
      "number of entities:  27657\n",
      "number of relations:  91\n",
      "number of train triplets:  243936\n",
      "number of int triplets:  242122\n",
      "number of val triplets:  114395\n",
      "number of ext triplets:  56367\n",
      "total time step:  40\n",
      "number of train time step:  28\n",
      "number of ext time step:  8\n",
      "number of val time step:  4\n",
      "=========preparing ro data============\n",
      "number of entities:  6947\n",
      "number of relations:  71\n",
      "number of train triplets:  43412\n",
      "number of int triplets:  42836\n",
      "number of val triplets:  20498\n",
      "number of ext triplets:  13544\n",
      "total time step:  40\n",
      "number of train time step:  28\n",
      "number of ext time step:  8\n",
      "number of val time step:  4\n",
      "=========preparing it data============\n",
      "number of entities:  23734\n",
      "number of relations:  84\n",
      "number of train triplets:  212419\n",
      "number of int triplets:  212826\n",
      "number of val triplets:  100263\n",
      "number of ext triplets:  51482\n",
      "total time step:  40\n",
      "number of train time step:  28\n",
      "number of ext time step:  8\n",
      "number of val time step:  4\n",
      "=========preparing no data============\n",
      "number of entities:  5571\n",
      "number of relations:  65\n",
      "number of train triplets:  31248\n",
      "number of int triplets:  31378\n",
      "number of val triplets:  18111\n",
      "number of ext triplets:  11231\n",
      "total time step:  40\n",
      "number of train time step:  28\n",
      "number of ext time step:  8\n",
      "number of val time step:  4\n",
      "=========preparing ru data============\n",
      "number of entities:  16828\n",
      "number of relations:  84\n",
      "number of train triplets:  128711\n",
      "number of int triplets:  128556\n",
      "number of val triplets:  59996\n",
      "number of ext triplets:  36205\n",
      "total time step:  40\n",
      "number of train time step:  28\n",
      "number of ext time step:  8\n",
      "number of val time step:  4\n",
      "=========preparing pt data============\n",
      "number of entities:  15548\n",
      "number of relations:  83\n",
      "number of train triplets:  121022\n",
      "number of int triplets:  121201\n",
      "number of val triplets:  56965\n",
      "number of ext triplets:  31407\n",
      "total time step:  40\n",
      "number of train time step:  28\n",
      "number of ext time step:  8\n",
      "number of val time step:  4\n",
      "=========preparing en data============\n",
      "number of entities:  34416\n",
      "number of relations:  105\n",
      "number of train triplets:  300655\n",
      "number of int triplets:  301501\n",
      "number of val triplets:  140636\n",
      "number of ext triplets:  70147\n",
      "total time step:  40\n",
      "number of train time step:  28\n",
      "number of ext time step:  8\n",
      "number of val time step:  4\n",
      "=========preparing es data============\n",
      "number of entities:  31808\n",
      "number of relations:  91\n",
      "number of train triplets:  285539\n",
      "number of int triplets:  285132\n",
      "number of val triplets:  135609\n",
      "number of ext triplets:  66275\n",
      "total time step:  40\n",
      "number of train time step:  28\n",
      "number of ext time step:  8\n",
      "number of val time step:  4\n",
      "=========preparing sl data============\n",
      "number of entities:  13250\n",
      "number of relations:  65\n",
      "number of train triplets:  60311\n",
      "number of int triplets:  59776\n",
      "number of val triplets:  21540\n",
      "number of ext triplets:  10094\n",
      "total time step:  40\n",
      "number of train time step:  28\n",
      "number of ext time step:  8\n",
      "number of val time step:  4\n",
      "=========preparing fr data============\n",
      "number of entities:  32546\n",
      "number of relations:  94\n",
      "number of train triplets:  289794\n",
      "number of int triplets:  290598\n",
      "number of val triplets:  137020\n",
      "number of ext triplets:  67166\n",
      "total time step:  40\n",
      "number of train time step:  28\n",
      "number of ext time step:  8\n",
      "number of val time step:  4\n"
     ]
    }
   ],
   "source": [
    "wiki_data_split = split_data(wiki_data, 40)\n",
    "dump_split_data(wiki_data_split, 'wiki')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Prepare experimental datasets:\n",
    "* source.csv: temporal triplets in source language;\n",
    "* target_train.csv: temporal triplets in target language for training;\n",
    "* target_val.csv: temporal triplets in target language for validation;\n",
    "* target_int.csv: temporal triplets in target language for interpolation;\n",
    "* target_ext.csv: temporal triplets in target language for extropolation;\n",
    "* alignment_train.csv: alignment pairs for training;\n",
    "* alignment_test.csv: alignment pairs for testing;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_language = ['en', 'fr']\n",
    "target_language = ['bg', 'da', 'de', 'es', 'hr', 'it', 'nl', 'no', 'pl', 'pt', 'ro', 'ru', 'sl']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data_split = load_split_data('wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map(data):\n",
    "    relations = list(set(data['train'].relation.unique()) | set(data['int'].relation.unique()))\n",
    "    relation2id = {item: index for index, item in enumerate(relations)}\n",
    "    entities = list(set(data['train'].subject.unique()) | set(data['int'].subject.unique()) | set(data['train'].object.unique()) | set(data['int'].object.unique()))\n",
    "    entity2id = {item: index for index, item in enumerate(entities)}\n",
    "    return relation2id, entity2id\n",
    "\n",
    "def reindex(data, relation2id, entity2id):\n",
    "    data = data[data.relation.isin(set(relation2id.keys()))]\n",
    "    data['relation'] = data['relation'].apply(lambda x: relation2id[x])\n",
    "    data['object'] = data['object'].apply(lambda x: entity2id[x])\n",
    "    data['subject'] = data['subject'].apply(lambda x: entity2id[x])\n",
    "    return data\n",
    "\n",
    "def experiment_data(source_lang, target_lang, data_split,data_name = 'wiki', alignment_ratio = 0.1):\n",
    "    source_data, target_data = data_split[source_lang].copy(), data_split[target_lang].copy()\n",
    "    relation2id, source_entity2id = get_map(source_data)\n",
    "    _, target_entity2id = get_map(target_data)\n",
    "    for key in source_data:\n",
    "        source_data[key] = reindex(source_data[key], relation2id, source_entity2id)\n",
    "    for key in target_data:\n",
    "        target_data[key] = reindex(target_data[key], relation2id, target_entity2id)\n",
    "    output_path = os.path.join(data_name, '{}-{}'.format(source_lang, target_lang))\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    source_data = pd.concat([source_data['train'], source_data['int']])\n",
    "    source_data.to_csv(os.path.join(output_path, 'source.csv'), index = False)\n",
    "    target_data['train'].to_csv(os.path.join(output_path, 'target_train.csv'), index = False)\n",
    "    target_data['int'].to_csv(os.path.join(output_path, 'target_int.csv'), index = False)\n",
    "    target_data['val'].to_csv(os.path.join(output_path, 'target_val.csv'), index = False)\n",
    "    target_data['ext'].to_csv(os.path.join(output_path, 'target_ext.csv'), index = False)\n",
    "\n",
    "    alignment = source_entity2id.keys() & target_entity2id.keys()\n",
    "    alignment_data = pd.DataFrame([[source_entity2id[item], target_entity2id[item]] for item in alignment], columns = ['source', 'target'])\n",
    "    alignment_data_train = alignment_data.sample(frac = alignment_ratio)\n",
    "    alignment_data_test = alignment_data.drop(alignment_data_train.index)\n",
    "    alignment_data_train.to_csv(os.path.join(output_path, 'alignment_train.csv'), index = False) \n",
    "    alignment_data_test.to_csv(os.path.join(output_path, 'alignment_test.csv'), index = False)\n",
    "    \n",
    "    print(\"=========preparing {}-{} data============\".format(source_lang, target_lang))\n",
    "    print(\"number of source entity: \", len(source_entity2id))\n",
    "    print(\"number of target entity: \", len(target_entity2id))\n",
    "    print(\"number of alignment: \", len(alignment))\n",
    "\n",
    "    return \"{}-{}\".format(source_lang, target_lang), source_data, target_data, alignment_data_train\n",
    "\n",
    "def prepare_our_data(source_language, target_language, data_split, data_name = 'wiki', alignment_ratio = 0.1):\n",
    "    task_data = dict()\n",
    "    for target_lang_ in target_language:\n",
    "        for source_lang_ in source_language:\n",
    "            task, source_data, target_data, alignment_data_train = experiment_data(source_lang_, target_lang_, wiki_data_split, data_name, alignment_ratio)\n",
    "            task_data[task] = {'source_data': source_data, 'target_data': target_data, 'alignment_data_train': alignment_data_train}\n",
    "    return task_data\n",
    "\n",
    "def prepare_OpenKE_data(task_data, data_name, output_path = \"../baseline/OpenKE/benchmarks/\"):\n",
    "    for task in task_data:\n",
    "        if task != 'en-it':\n",
    "            continue\n",
    "        source_data = task_data[task]['source_data']\n",
    "        target_data = task_data[task]['target_data']\n",
    "        alignment_data_train = task_data[task]['alignment_data_train']\n",
    "        output_dir = os.path.join(output_path, data_name, task)\n",
    "        \n",
    "        entity_base = len(set(source_data.subject.unique()) | set(source_data.object.unique()))\n",
    "        print(entity_base)\n",
    "        relation_base = len(set(source_data.relation.unique()))\n",
    "\n",
    "        train_data = [[item['subject'], item['object'], item['relation']] for _, item in source_data.iterrows()] + \\\n",
    "                        [[item['subject'] + entity_base, item['object'] + entity_base, item['relation']] for _, item in target_data['train'].iterrows()] + \\\n",
    "                            [[item['source'], item['target'] + entity_base, relation_base] for _, item in alignment_data_train.iterrows()]\n",
    "        val_data = [[item['subject'] + entity_base, item['object'] + entity_base, item['relation']] for _, item in target_data['val'].iterrows()]\n",
    "        test_data = [[item['subject'] + entity_base, item['object'] + entity_base, item['relation']] for _, item in target_data['ext'].iterrows()]\n",
    "        test_int_data = [[item['subject'] + entity_base, item['object'] + entity_base, item['relation']] for _, item in target_data['int'].iterrows()]\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        for data, name in zip([train_data, val_data, test_data, test_int_data], ['train2id.txt', 'valid2id.txt', 'test2id.txt', 'test_int2id.txt']):\n",
    "            with open(os.path.join(output_dir, name), 'w') as f:\n",
    "                f.write('{}\\n'.format(len(data)))\n",
    "                for item in data:\n",
    "                    f.write('{} {} {}\\n'.format(item[0], item[1], item[2]))\n",
    "                f.close()\n",
    "        \n",
    "        \n",
    "        number_of_relation = len(set(source_data.relation.unique())) + 1\n",
    "        number_of_entity = len(get_map(target_data)[1]) + len(set(source_data.subject.unique()) | set(source_data.object.unique()))\n",
    "        print(len(train_data[:][0]), len(val_data), len(test_data), len(test_int_data))\n",
    "        with open(os.path.join(output_dir, 'entity2id.txt'), 'w') as f:\n",
    "            f.write('{}\\n'.format(number_of_entity))\n",
    "            for i in range(number_of_entity):\n",
    "                f.write('{}\\t{}\\n'.format(i,i))\n",
    "            f.close()\n",
    "        with open(os.path.join(output_dir, 'relation2id.txt'), 'w') as f:\n",
    "            f.write('{}\\n'.format(number_of_relation))\n",
    "            for i in range(number_of_relation):\n",
    "                f.write('{}\\t{}\\n'.format(i,i))\n",
    "            f.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========preparing en-bg data============\n",
      "number of source entity:  34416\n",
      "number of target entity:  3508\n",
      "number of alignment:  3508\n",
      "=========preparing fr-bg data============\n",
      "number of source entity:  32546\n",
      "number of target entity:  3508\n",
      "number of alignment:  3504\n",
      "=========preparing en-da data============\n",
      "number of source entity:  34416\n",
      "number of target entity:  15710\n",
      "number of alignment:  15690\n",
      "=========preparing fr-da data============\n",
      "number of source entity:  32546\n",
      "number of target entity:  15710\n",
      "number of alignment:  15447\n",
      "=========preparing en-de data============\n",
      "number of source entity:  34416\n",
      "number of target entity:  27657\n",
      "number of alignment:  27621\n",
      "=========preparing fr-de data============\n",
      "number of source entity:  32546\n",
      "number of target entity:  27657\n",
      "number of alignment:  27289\n",
      "=========preparing en-es data============\n",
      "number of source entity:  34416\n",
      "number of target entity:  31808\n",
      "number of alignment:  31771\n",
      "=========preparing fr-es data============\n",
      "number of source entity:  32546\n",
      "number of target entity:  31808\n",
      "number of alignment:  31068\n",
      "=========preparing en-hr data============\n",
      "number of source entity:  34416\n",
      "number of target entity:  3161\n",
      "number of alignment:  3161\n",
      "=========preparing fr-hr data============\n",
      "number of source entity:  32546\n",
      "number of target entity:  3161\n",
      "number of alignment:  3161\n",
      "=========preparing en-it data============\n",
      "number of source entity:  34416\n",
      "number of target entity:  23734\n",
      "number of alignment:  23715\n",
      "=========preparing fr-it data============\n",
      "number of source entity:  32546\n",
      "number of target entity:  23734\n",
      "number of alignment:  23619\n",
      "=========preparing en-nl data============\n",
      "number of source entity:  34416\n",
      "number of target entity:  30872\n",
      "number of alignment:  30854\n",
      "=========preparing fr-nl data============\n",
      "number of source entity:  32546\n",
      "number of target entity:  30872\n",
      "number of alignment:  30111\n",
      "=========preparing en-no data============\n",
      "number of source entity:  34416\n",
      "number of target entity:  5571\n",
      "number of alignment:  5564\n",
      "=========preparing fr-no data============\n",
      "number of source entity:  32546\n",
      "number of target entity:  5571\n",
      "number of alignment:  5537\n",
      "=========preparing en-pl data============\n",
      "number of source entity:  34416\n",
      "number of target entity:  16706\n",
      "number of alignment:  16699\n",
      "=========preparing fr-pl data============\n",
      "number of source entity:  32546\n",
      "number of target entity:  16706\n",
      "number of alignment:  16598\n",
      "=========preparing en-pt data============\n",
      "number of source entity:  34416\n",
      "number of target entity:  15548\n",
      "number of alignment:  15545\n",
      "=========preparing fr-pt data============\n",
      "number of source entity:  32546\n",
      "number of target entity:  15548\n",
      "number of alignment:  15366\n",
      "=========preparing en-ro data============\n",
      "number of source entity:  34416\n",
      "number of target entity:  6947\n",
      "number of alignment:  6944\n",
      "=========preparing fr-ro data============\n",
      "number of source entity:  32546\n",
      "number of target entity:  6947\n",
      "number of alignment:  6895\n",
      "=========preparing en-ru data============\n",
      "number of source entity:  34416\n",
      "number of target entity:  16828\n",
      "number of alignment:  16770\n",
      "=========preparing fr-ru data============\n",
      "number of source entity:  32546\n",
      "number of target entity:  16828\n",
      "number of alignment:  16325\n",
      "=========preparing en-sl data============\n",
      "number of source entity:  34416\n",
      "number of target entity:  13250\n",
      "number of alignment:  13250\n",
      "=========preparing fr-sl data============\n",
      "number of source entity:  32546\n",
      "number of target entity:  13250\n",
      "number of alignment:  13212\n"
     ]
    }
   ],
   "source": [
    "task_data = prepare_our_data(source_language, target_language, wiki_data_split, data_name = 'wiki', alignment_ratio = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_task_data(task_data, 'wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34416\n",
      "3 100263 51482 212826\n"
     ]
    }
   ],
   "source": [
    "task_data = load_task_data('wiki')\n",
    "prepare_OpenKE_data(task_data, data_name = 'wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for KEnS, AlignKGC, SS-AGA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def data_transfer(path):\n",
    "    out_path = \"/home/ec2-user/quic-efs/user/ruijiew/TKGC/baseline/ss-aga-kgc/dataset\"\n",
    "    column_list = [\"subject\", \"relation\", \"object\"]\n",
    "    \n",
    "    for task in os.listdir(path):\n",
    "        if not os.path.isdir(os.path.join(path, task)):\n",
    "            continue\n",
    "        source, target = task.split('-')[0], task.split('-')[1]\n",
    "        \n",
    "        if not os.path.exists(os.path.join(out_path, task)):\n",
    "            os.mkdir(os.path.join(out_path, task))\n",
    "        \n",
    "        source_data = pd.read_csv(os.path.join(path, task, 'source.csv'))\n",
    "        target_train_data = pd.read_csv(os.path.join(path, task, 'target_train.csv'))\n",
    "        target_val_data = pd.read_csv(os.path.join(path, task, 'target_val.csv'))\n",
    "        target_test_data = pd.read_csv(os.path.join(path, task, 'target_ext.csv'))\n",
    "        alignment_data = pd.read_csv(os.path.join(path, task, 'alignment_train.csv'))\n",
    "        try:\n",
    "            emb_data = np.load(os.path.join(path, task, 'ent_embedding.npy'))\n",
    "            np.save(os.path.join(out_path, task, 'entity_embeddings.npy'), emb_data)\n",
    "            \n",
    "            rel_data = np.load(os.path.join(path, task, 'rel_embedding.npy'))\n",
    "            with open(os.path.join(out_path, task, 'relations.txt'), 'w') as f:\n",
    "                for i in range(rel_data.shape[0]):\n",
    "                    f.write('{}\\n'.format(i))\n",
    "                f.close()\n",
    "        except:\n",
    "            pass\n",
    "        if not os.path.exists(os.path.join(out_path, task, 'kg')):\n",
    "            os.mkdir(os.path.join(out_path, task, 'kg'))\n",
    "        source_data[column_list].to_csv(os.path.join(out_path, task, 'kg', \"{}-train.tsv\".format(source)), sep = '\\t', header = False, index = False)\n",
    "        source_data[column_list].to_csv(os.path.join(out_path, task, 'kg', \"{}-val.tsv\".format(source)), sep = '\\t', header = False, index = False)\n",
    "        source_data[column_list].to_csv(os.path.join(out_path, task, 'kg', \"{}-test.tsv\".format(source)), sep = '\\t', header = False, index = False)\n",
    "        target_train_data[column_list].to_csv(os.path.join(out_path, task, 'kg', \"{}-train.tsv\".format(target)), sep = '\\t', header = False, index = False)\n",
    "        target_val_data[column_list].to_csv(os.path.join(out_path, task, 'kg', \"{}-val.tsv\".format(target)), sep = '\\t', header = False, index = False)\n",
    "        target_test_data[column_list].to_csv(os.path.join(out_path, task, 'kg', \"{}-test.tsv\".format(target)), sep = '\\t', header = False, index = False)\n",
    "        \n",
    "        if not os.path.exists(os.path.join(out_path, task, 'seed_alignlinks')):\n",
    "            os.mkdir(os.path.join(out_path, task, 'seed_alignlinks'))\n",
    "        alignment_data.to_csv(os.path.join(out_path, task, 'seed_alignlinks', \"{}-{}.tsv\".format(source, target)), sep = '\\t', header = False, index = False)\n",
    "        \n",
    "        if not os.path.exists(os.path.join(out_path, task, 'entity')):\n",
    "            os.mkdir(os.path.join(out_path, task, 'entity'))\n",
    "       \n",
    "        with open(os.path.join(out_path, task, 'entity', \"{}.tsv\".format(source)), 'w') as f:\n",
    "            source_ent_num = len(set(source_data.subject.unique()) | set(source_data.object.unique()))\n",
    "            for i in range(source_ent_num):\n",
    "                f.write('{}\\n'.format(i))\n",
    "            f.close()\n",
    "            \n",
    "        with open(os.path.join(out_path, task, 'entity', \"{}.tsv\".format(target)), 'w') as f:\n",
    "            target_ent_num = emb_data.shape[0] - source_ent_num\n",
    "            for i in range(target_ent_num):\n",
    "                f.write('{}\\n'.format(i))\n",
    "            f.close()\n",
    "        \n",
    "data_transfer(\"./wiki\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ss-aga')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f560f0eadab7070ccacce5114879206219b721432fffe45afdc25b26c4c91e89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
